{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat(r'D:\\fUSI\\Code base\\test_data\\test\\fr1.mat')\n",
    "print(mat.keys())\n",
    "print(mat['x'].shape)\n",
    "print(mat['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "class fUSI(Dataset):\n",
    "    def __init__(self, folder_path, mat_key='y', transform=None):\n",
    "        self.mat_files = sorted(glob(os.path.join(folder_path, '*.mat')))\n",
    "        self.mat_key = mat_key\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mat_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mat_file = self.mat_files[idx]\n",
    "        data = scipy.io.loadmat(mat_file)\n",
    "        img = data[self.mat_key].astype(np.float32)  # e.g. key = 'y'\n",
    "\n",
    "        # Add channel dimension if needed\n",
    "        if img.ndim == 2:\n",
    "            img = np.expand_dims(img, axis=0)  # (1, H, W)\n",
    "        elif img.ndim == 3 and img.shape[-1] <= 3:\n",
    "            img = img.transpose(2, 0, 1)  # (C, H, W)\n",
    "\n",
    "        img_tensor = torch.tensor(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "\n",
    "        return img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c103a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "folder_path = r'D:\\fUSI\\Code base\\test_data\\test'\n",
    "fUSI_data = fUSI(folder_path, mat_key='y')\n",
    "dataloader = DataLoader(fUSI_data, batch_size=4, shuffle=True)\n",
    "for batch in dataloader:\n",
    "    print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class KSparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, k_sparsity):\n",
    "        super(KSparseAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder: Fully connected layers\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Decoder: Fully connected layers\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.k_sparsity = k_sparsity  # sparsity level\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding: Compute sparse code\n",
    "        sparse_code = self.encoder(x)\n",
    "        \n",
    "        # Apply sparsity\n",
    "        sparse_code = self.apply_sparsity(sparse_code)\n",
    "        \n",
    "        # Decoding: Reconstruct input\n",
    "        reconstructed = self.decoder(sparse_code)\n",
    "        \n",
    "        return reconstructed, sparse_code\n",
    "\n",
    "    def apply_sparsity(self, sparse_code):\n",
    "        # Apply k-sparsity (set all but top k elements to 0)\n",
    "        topk_values, topk_indices = torch.topk(sparse_code.abs(), self.k_sparsity, dim=-1)\n",
    "        sparse_code = torch.zeros_like(sparse_code)\n",
    "        sparse_code.scatter_(1, topk_indices, topk_values)\n",
    "        return sparse_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6587d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(original, reconstructed):\n",
    "    return nn.MSELoss()(reconstructed, original)\n",
    "\n",
    "def sparsity_loss(sparse_code, k_sparsity):\n",
    "    # Penalty for sparsity violation (encourages k-sparsity)\n",
    "    return torch.sum(torch.abs(sparse_code)) - k_sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_sparse_autoencoder(model, dataloader, num_epochs=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Flatten the image data (assuming data is in shape [B, C, H, W])\n",
    "            data_flat = data.view(data.size(0), -1)  # B x (C * H * W)\n",
    "            \n",
    "            # Forward pass: get reconstructed output and sparse code\n",
    "            reconstructed, sparse_code = model(data_flat)\n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            rec_loss = reconstruction_loss(data_flat, reconstructed)\n",
    "            \n",
    "            # Compute sparsity loss\n",
    "            sparsity_penalty = sparsity_loss(sparse_code, model.k_sparsity)\n",
    "            \n",
    "            # Total loss (combine both)\n",
    "            loss = rec_loss + sparsity_penalty\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n",
    "        visualize_dictionary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05473fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Set dimensions for the model\n",
    "input_dim = 96 * 96  # assuming input images are 64x64\n",
    "hidden_dim = 512     # size of the hidden sparse code\n",
    "k_sparsity = 10      # top-k non-zero elements in the sparse code\n",
    "\n",
    "# Initialize model\n",
    "model = KSparseAutoencoder(input_dim, hidden_dim, k_sparsity)\n",
    "\n",
    "# Assuming you already have a DataLoader for your dataset\n",
    "# Train the model\n",
    "train_k_sparse_autoencoder(model, dataloader, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7116cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_dictionary(model):\n",
    "    # Extract the weights of the encoder layer (first layer)\n",
    "    dictionary = model.encoder.weight.data.cpu().numpy()\n",
    "\n",
    "    # Reshape to display each learned feature (assuming each feature is a flattened image)\n",
    "    num_features = dictionary.shape[0]\n",
    "    feature_size = int(np.sqrt(dictionary.shape[1]))  # assuming square images, adjust if needed\n",
    "    dictionary_reshaped = dictionary.reshape(num_features, feature_size, feature_size)\n",
    "\n",
    "    # Plot learned dictionary (each feature)\n",
    "    num_cols = 8\n",
    "    num_rows = num_features // num_cols\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 1.5, num_rows * 1.5))\n",
    "\n",
    "    for i in range(num_features):\n",
    "        ax = axs[i // num_cols, i % num_cols]\n",
    "        ax.imshow(dictionary_reshaped[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5e8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe96f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from overcomplete.sae import TopKSAE, train_sae\n",
    "\n",
    "# Define your activation tensor (ensure N and d are set)\n",
    "N, d = 40, 9216  # Example with 10,000 samples of 784-dim activations (e.g., MNIST flattened)\n",
    "Activations = torch.randn(N, d).to('cpu')\n",
    "\n",
    "# Initialize Sparse Autoencoder\n",
    "sae = TopKSAE(d, nb_concepts=16_000, top_k=10, device='cpu')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=5e-4)\n",
    "\n",
    "# Loss function\n",
    "def criterion(x, x_hat, pre_codes, codes, dictionary):\n",
    "    mse = (x - x_hat).square().mean()\n",
    "    return mse\n",
    "\n",
    "# Training the model\n",
    "logs = train_sae(sae, dataloader, criterion, optimizer, nb_epochs=20, device='cpu')\n",
    "\n",
    "# Visualizing the learned dictionary\n",
    "def visualize_dictionary(sae, num_atoms=10):\n",
    "    dictionary = sae.encoder.weight.data.cpu().numpy()  # Extract learned dictionary\n",
    "    fig, axes = plt.subplots(1, num_atoms, figsize=(num_atoms * 2, 2))\n",
    "    \n",
    "    for i in range(num_atoms):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(dictionary[i].reshape(28, 28), cmap='gray')  # Assuming 28x28 for MNIST\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# After training, visualize the learned dictionary\n",
    "visualize_dictionary(sae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a94cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from overcomplete.sae import TopKSAE, train_sae\n",
    "\n",
    "# Dataset\n",
    "class fUSI(Dataset):\n",
    "    def __init__(self, folder_path, mat_key='y', transform=None):\n",
    "        self.mat_files = sorted(glob(os.path.join(folder_path, '*.mat')))\n",
    "        self.mat_key = mat_key\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mat_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mat_file = self.mat_files[idx]\n",
    "        data = scipy.io.loadmat(mat_file)\n",
    "        img = data[self.mat_key].astype(np.float32)\n",
    "\n",
    "        if img.ndim == 2:\n",
    "            img = np.expand_dims(img, axis=0)  # (1, H, W)\n",
    "        elif img.ndim == 3 and img.shape[-1] <= 3:\n",
    "            img = img.transpose(2, 0, 1)  # (C, H, W)\n",
    "\n",
    "        img_tensor = torch.tensor(img)\n",
    "\n",
    "        # 🔧 Fix: Use reshape instead of view for non-contiguous tensor\n",
    "        img_tensor = img_tensor.reshape(-1)\n",
    "\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "\n",
    "        return img_tensor\n",
    "\n",
    "# Initialize Dataset and Dataloader\n",
    "folder_path = r\"D:\\fUSI\\Code base\\test_data\\test\"  # ⬅️ Update this path\n",
    "fUSI_data = fUSI(folder_path, mat_key='y')\n",
    "dataloader = DataLoader(fUSI_data, batch_size=1, shuffle=True)\n",
    "\n",
    "# Set dimensions\n",
    "sample = next(iter(dataloader))\n",
    "input_dim = sample.shape[1]  # should be H * W\n",
    "\n",
    "# Define SAE model\n",
    "sae = TopKSAE(input_dim, nb_concepts=16000, top_k=10, device='cpu')\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=5e-4)\n",
    "\n",
    "# Define loss function\n",
    "def criterion(x, x_hat, pre_codes, codes, dictionary):\n",
    "    return (x - x_hat).square().mean()\n",
    "\n",
    "# Train the model\n",
    "logs = train_sae(sae, dataloader, criterion, optimizer, nb_epochs=20, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned dictionary\n",
    "def visualize_dictionary(sae, num_atoms, img_size=(96, 96)):\n",
    "    dictionary = sae.get_dictionary().detach().cpu().numpy()  # Shape: (nb_concepts, input_dim)\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_atoms):\n",
    "        atom = dictionary[i].reshape(img_size)\n",
    "        plt.subplot(1, num_atoms, i + 1)\n",
    "        plt.imshow(atom, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"TopKSAE Learned Dictionary Atoms\")\n",
    "    plt.show()\n",
    "\n",
    "# Show dictionary atoms\n",
    "visualize_dictionary(sae, num_atoms=10, img_size=(96, 96)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f140fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one sample from the dataset\n",
    "sample_input = next(iter(dataloader))  # shape: (batch_size, input_dim)\n",
    "sample_input = sample_input[0].unsqueeze(0)  # select first image in batch, keep batch dim\n",
    "\n",
    "# Move to device\n",
    "sample_input = sample_input.to(sae.device)\n",
    "\n",
    "# Forward pass through SAE\n",
    "with torch.no_grad():\n",
    "    z_pre, z, x_hat = sae(sample_input)  # x_hat is the reconstructed input\n",
    "\n",
    "# Convert tensors to numpy for visualization\n",
    "original = sample_input.cpu().numpy().reshape(1, 96, 96)  # assuming original shape was (1, H, W)\n",
    "reconstructed = x_hat.cpu().numpy().reshape(1, 96, 96)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original[0], cmap='gray')\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed[0], cmap='gray')\n",
    "plt.title(\"Reconstructed Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sae.state_dict(), \"trained_sae_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
